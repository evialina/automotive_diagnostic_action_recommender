{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Claims & Vehicle Fault-based Diagnostic Action Prediction (CVFDA) Model\n",
    "\n",
    "This component uses a vehicle's fault and claim history to predict the most suitable diagnostic actions to address a particular fault. This prediction doesn't consider the sequence in which the actions should be executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-08-01T23:35:34.951869Z",
     "end_time": "2023-08-01T23:35:46.897728Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-01 23:35:38.944943: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datetime import datetime\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Constants\n",
    "EMBEDDING_DIM = 50\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 50\n",
    "VALIDATION_SPLIT = 0.2\n",
    "SEED = 42"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-01T23:35:46.901041Z",
     "end_time": "2023-08-01T23:35:46.910657Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Prepared Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-08-01T23:35:46.914053Z",
     "end_time": "2023-08-01T23:35:46.945355Z"
    }
   },
   "outputs": [],
   "source": [
    "data_df = pd.read_csv('./data_out/prepared_data.csv')\n",
    "data_df = data_df.iloc[:, 1:] # remove index column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Encode Categorical Data Features\n",
    "\n",
    "Encode categorical features using a label encoding technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-30T22:49:56.503613Z",
     "start_time": "2023-07-30T22:49:56.460849Z"
    }
   },
   "outputs": [],
   "source": [
    "categorical_features = ['model', 'modelyear', 'driver', 'plant', 'engine', 'transmission', 'module', 'dtcbase', 'faulttype',\n",
    "                      'dtcfull', 'year', 'month', 'dayOfWeek', 'weekOfYear', 'season', 'i_original_vfg_code',\n",
    "                      'softwarepartnumber', 'hardwarepartnumber', 'i_p_css_code', 'i_original_ccc_code',\n",
    "                      'i_original_function_code', 'i_original_vrt_code', 'i_current_vfg_code', 'i_current_function_code',\n",
    "                      'i_current_vrt_code',\t'i_cpsc_code', 'i_cpsc_vfg_code', 'i_css_code', 'v_transmission_code',\n",
    "                      'v_drive_code', 'v_engine_code', 'ic_repair_dealer_id', 'ic_eng_part_number', 'ic_serv_part_number',\n",
    "                      'ic_part_suffix', 'ic_part_base', 'ic_part_prefix', 'ic_causal_part_id', 'ic_repair_country_code']\n",
    "\n",
    "# Convert each categorical feature to integer encoding\n",
    "label_encoder = LabelEncoder()\n",
    "for feature in categorical_features:\n",
    "    data_df[feature] = label_encoder.fit_transform(data_df[feature])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Variable Embedding\n",
    "\n",
    "Categorical variables are converted to dense representations (embedding vectors) instead of sparse dummy variables. For each category of a categorical variable, an embedding vector (feature vector) is obtained. The same process is applied for continuous variables using a Multi-Layer Perceptron (MLP) to reduce the dimensions of the continuous variables. After obtaining the embedding vectors for each variable, the embedding vectors for user variables and item variables are concatenated separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-30T22:49:58.014441Z",
     "start_time": "2023-07-30T22:49:56.513623Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dimension for our embeddings.\n",
    "# This is may need to be tuned for optimal performance.\n",
    "embedding_dim = 50\n",
    "input_layers = []\n",
    "embedding_layers = []\n",
    "\n",
    "for col in categorical_features:\n",
    "    num_unique_categories = data_df[col].nunique()\n",
    "\n",
    "    # Create input layer for each category\n",
    "    input_layer = keras.layers.Input(shape=(1,), name=f\"{col}_input\")\n",
    "    input_layers.append(input_layer)\n",
    "\n",
    "    # Create embedding layer for each category\n",
    "    embedding = keras.layers.Embedding(num_unique_categories, embedding_dim, input_length=1, name=f\"{col}_embedding\")(input_layer)\n",
    "\n",
    "    # Flatten the embedding layer\n",
    "    embedding_flatten = keras.layers.Flatten()(embedding)\n",
    "    embedding_layers.append(embedding_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-30T22:49:58.081527Z",
     "start_time": "2023-07-30T22:49:57.993653Z"
    }
   },
   "outputs": [],
   "source": [
    "# list of numerical columns\n",
    "numerical_features = ['elapsedTimeSec', 'timeSinceLastActivitySec', 'odomiles', 'vehicleAgeAtSession',\n",
    "            'daysSinceWarrantyStart', 'i_mileage', 'i_time_in_service', 'i_months_in_service']\n",
    "\n",
    "# Define the input layer for the numerical features\n",
    "num_input = keras.layers.Input(shape=(len(numerical_features),), name='numerical_input')\n",
    "\n",
    "# Pass the numerical inputs through a MLP\n",
    "hidden1 = keras.layers.Dense(128, activation='relu')(num_input)\n",
    "hidden2 = keras.layers.Dense(64, activation='relu')(hidden1)\n",
    "num_output = keras.layers.Dense(32, activation='relu')(hidden2)  # This is the embedding of the continuous features\n",
    "\n",
    "# Append num_output to the list of embedding layers\n",
    "embedding_layers.append(num_output)\n",
    "# Concatenate all the embedding vectors\n",
    "embeddings_concat = tf.keras.layers.concatenate(embedding_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Input Reshaping to Image Like\n",
    "\n",
    "The Conv2D layer expects an input with 4 dimensions - typically (batch_size, height, width, channels). In the context of image processing, height and width would be the dimensions of the image, and channels would refer to color channels (like RGB). In this case, since we're not working with images, height and width don't represent spatial dimensions, but we still need to reshape our data to match what the Conv2D layer expects.\n",
    "\n",
    "Step-by-step:\n",
    "1. We get the total number of features after concatenating all our embeddings, which is 1982.\n",
    "2. We calculate the nearest square number that's greater than or equal to this number, which is 2025 (since 45 * 45 = 2025). This is done so that we can reshape our data into a square \"image\". The rounding up happens to make sure that when we reshape the tensor, we have enough slots to accommodate all our features.\n",
    "3. We reshape our data to have a height and width equal to this square root. The -1 in our reshape operation means that this dimension will be calculated automatically based on the total size of the tensor and the other dimensions. So, the reshaped tensor has shape (None, 45, 45).\n",
    "4. Finally, we expand the dimensions of our data to add a channel dimension. This is done to meet the requirements of the Conv2D layer, which expects input with 4 dimensions: (batch_size, height, width, channels). After this step, the shape of our data is (None, 45, 45, 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-30T22:49:58.187706Z",
     "start_time": "2023-07-30T22:49:58.087747Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'Original embeddings shape: {embeddings_concat.shape}')\n",
    "\n",
    "total_features = embeddings_concat.shape[1] # the total number of features after concatenation\n",
    "sqrt_features = int(np.ceil(np.sqrt(total_features))) # the nearest square number greater than total_features\n",
    "\n",
    "flattened = tf.keras.layers.Flatten()(embeddings_concat)\n",
    "dense_for_reshape = tf.keras.layers.Dense(sqrt_features*sqrt_features, activation='relu')(flattened)\n",
    "\n",
    "# Reshape\n",
    "embeddings_reshaped = tf.keras.layers.Reshape((sqrt_features, sqrt_features, 1))(dense_for_reshape)\n",
    "print(f'Reshaped embeddings shape: {embeddings_reshaped.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution with Cross Convolutional Filters\n",
    "\n",
    "A convolution operation is performed using cross convolutional filters on the outer product of user and item embedding vectors, which serve as a feature map of interactions. This step effectively models the interactions between users and items.\n",
    "\n",
    "The Cross Convolutional Layer consists of two separate convolutional operations: one with vertical filters and one with horizontal filters. The outputs of these operations are added together element-wise to create the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-30T22:49:58.213734Z",
     "start_time": "2023-07-30T22:49:58.164903Z"
    }
   },
   "outputs": [],
   "source": [
    "# The CrossConv2D layer applies a vertical and a horizontal convolution operation and then adds the results.\n",
    "@keras.saving.register_keras_serializable('models')\n",
    "class CrossConv2D(Layer):\n",
    "    def __init__(self, filters, kernel_size, **kwargs):\n",
    "        super(CrossConv2D, self).__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters, (kernel_size[0], 1), activation='relu', padding='same')\n",
    "        self.conv2 = tf.keras.layers.Conv2D(filters, (1, kernel_size[1]), activation='relu', padding='same')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        conv1_output = self.conv1(inputs)\n",
    "        conv2_output = self.conv2(inputs)\n",
    "        return conv1_output + conv2_output\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        config = {\"filters\": self.filters, \"kernel_size\": self.kernel_size}\n",
    "        return {**base_config, **config}\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The softmax activation function is used in the final layer, which is suitable for multiclass classification problems. The model is compiled with the Adam optimizer and categorical crossentropy loss function, which are good defaults for a classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-30T22:50:05.327918Z",
     "start_time": "2023-07-30T22:49:58.181552Z"
    }
   },
   "outputs": [],
   "source": [
    "conv1 = CrossConv2D(filters=32, kernel_size=(3, 3))(embeddings_reshaped)\n",
    "batch_norm1 = keras.layers.BatchNormalization()(conv1)\n",
    "activation1 = tf.keras.activations.relu(batch_norm1)\n",
    "pooling1 = keras.layers.MaxPooling2D(pool_size=(2, 2))(activation1)\n",
    "dropout1 = keras.layers.Dropout(0.25)(pooling1)\n",
    "\n",
    "conv2 = CrossConv2D(filters=64, kernel_size=(3, 3))(dropout1)\n",
    "batch_norm2 = keras.layers.BatchNormalization()(conv2)\n",
    "activation2 = tf.keras.activations.relu(batch_norm2)\n",
    "pooling2 = keras.layers.MaxPooling2D(pool_size=(2, 2))(activation2)\n",
    "dropout2 = keras.layers.Dropout(0.25)(pooling2)\n",
    "\n",
    "flatten = keras.layers.Flatten()(dropout2)\n",
    "dense1 = keras.layers.Dense(256, activation='relu')(flatten)\n",
    "dropout3 = keras.layers.Dropout(0.5)(dense1)\n",
    "\n",
    "# Get the number of unique 'otxsequence' and adjust the output layer\n",
    "num_unique_otxsequence = data_df['otxsequence'].nunique()\n",
    "output_layer = keras.layers.Dense(num_unique_otxsequence, activation='softmax')(dropout3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Model(inputs=input_layers + [num_input], outputs=[output_layer])\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "plot_model(model, to_file='models/cvf-da_layers.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-30T22:50:05.333287Z",
     "start_time": "2023-07-30T22:50:05.324971Z"
    }
   },
   "outputs": [],
   "source": [
    "features = data_df.drop(columns='otxsequence')\n",
    "encoded_otxsequence = LabelEncoder().fit_transform(data_df['otxsequence'])\n",
    "target = keras.utils.to_categorical(encoded_otxsequence)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# For training, we need to separate the categorical and numerical features as we have separate input layers for them\n",
    "train_input = [X_train[feature].values for feature in categorical_features] + [X_train[numerical_features].values]\n",
    "test_input = [X_test[feature].values for feature in categorical_features] + [X_test[numerical_features].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-30T22:50:33.108410Z",
     "start_time": "2023-07-30T22:50:05.351630Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "early_stopping_monitor = keras.callbacks.EarlyStopping(\n",
    "    monitor='accuracy',\n",
    "    min_delta=0.001,  # minimum change to qualify as an improvement\n",
    "    patience=5,  # number of epochs with no improvement after which training will be stopped\n",
    "    verbose=1,\n",
    "    mode='auto',\n",
    "    baseline=None,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_input,\n",
    "    y_train,\n",
    "    batch_size=128,\n",
    "    epochs=50,\n",
    "    validation_data=(test_input, y_test),\n",
    "    callbacks=[early_stopping_monitor]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-30T22:50:33.351271Z",
     "start_time": "2023-07-30T22:50:33.120088Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate the performance of the model on the test data\n",
    "loss, accuracy = model.evaluate(test_input, y_test)\n",
    "\n",
    "print(\"Model Test Loss:\", loss)\n",
    "print(\"Model Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-30T22:50:34.409407Z",
     "start_time": "2023-07-30T22:50:33.359628Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('CVF-DA Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('CVF-DA Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-30T22:50:36.005723Z",
     "start_time": "2023-07-30T22:50:34.332219Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save the model for later use\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model.save(f'models_test/cvf-da_{timestamp}.keras')"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,auto:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
