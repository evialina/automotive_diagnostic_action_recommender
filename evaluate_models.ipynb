{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from data_preprocessing import load_data\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "from collections import Counter\n",
    "from sklearn.metrics import ndcg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "y_test = load_data('models_test/y_test.csv')\n",
    "test_input = load_data('models_test/test_input.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def filter_minor_classes(test_input, y):\n",
    "    # count the number of instances in each class\n",
    "    class_counts = Counter(y)\n",
    "\n",
    "    # get the classes with only 1 instance\n",
    "    small_classes = [k for k, v in class_counts.items() if v < 2]\n",
    "\n",
    "    # remove these classes from your dataset\n",
    "    mask = np.logical_not(np.isin(y, small_classes))\n",
    "\n",
    "    X = [array[mask] for array in test_input]\n",
    "    y = y[mask]\n",
    "\n",
    "    # Convert back to numpy arrays and ensure the type is float32\n",
    "    return X, y\n",
    "\n",
    "def evaluate_model(model, test_input, y_test):\n",
    "    # Evaluate the performance of the model on the test data\n",
    "    loss, accuracy = model.evaluate(test_input, y_test)\n",
    "    print(f\"Test Loss: {loss}\")\n",
    "    print(f\"Test Accuracy: {accuracy}\")\n",
    "    print(f\">>>>>>>> test_input original: {test_input}\")\n",
    "    print(f\">>>>>>>> y_test original: {y_test}\")\n",
    "\n",
    "    # Filter out classes with less than 2 instances\n",
    "    y_test_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "    # Reshape the 1D arrays in test_input to 2D\n",
    "    test_input = [arr.reshape(-1, 1) if len(arr.shape) == 1 else arr for arr in test_input]\n",
    "    # Concatenate all arrays in test_input along the last axis\n",
    "    test_input = np.concatenate(test_input, axis=-1)\n",
    "\n",
    "    test_input_filtered, y_test_filtered = filter_minor_classes(test_input, y_test_classes)\n",
    "\n",
    "    print(f\">>>>>>>> test_input: {test_input}\")\n",
    "    print(f\">>>>>>>> y_test: {y_test}\")\n",
    "\n",
    "    # Calculate the predicted class as the one with highest probability\n",
    "    y_pred = model.predict(test_input_filtered)\n",
    "    y_pred_class = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    # Calculate metrics\n",
    "    precision = precision_score(y_test_filtered, y_pred_class, average='weighted')\n",
    "    recall = recall_score(y_test_filtered, y_pred_class, average='weighted')\n",
    "    f1 = f1_score(y_test_filtered, y_pred_class, average='weighted')\n",
    "    roc_auc = roc_auc_score(y_test_filtered, y_pred, multi_class='ovr')\n",
    "    ndcg = ndcg_score(y_test_filtered, y_pred)\n",
    "\n",
    "    print(\"Precision: \", precision)\n",
    "    print(\"Recall: \", recall)\n",
    "    print(\"F1 Score: \", f1)\n",
    "    print(\"ROC AUC Score: \", roc_auc)\n",
    "    print(\"NDCG: \", ndcg)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
