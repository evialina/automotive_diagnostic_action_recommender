{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "380bb861-1dc8-4f70-827a-c2c34faff4a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T17:43:56.500770Z",
     "start_time": "2023-07-22T17:43:56.261985Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.59 s, sys: 1.49 s, total: 3.08 s\n",
      "Wall time: 1.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b156823f",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0124adc2-0616-4b88-a4f7-0d94fa5db5e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T17:44:50.283879Z",
     "start_time": "2023-07-22T17:43:56.281066Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    diagnostic_df = pd.read_csv(f'data/chunks_2m/{filename}',  \n",
    "                                 dtype={'dtcbase': 'object', 'dtcfull': 'object', 'odomiles': 'float64'}, \n",
    "                                 low_memory=False)\n",
    "    return diagnostic_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b00051",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Feature Engineering: Consultation ID\n",
    "# !Add description!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acdb9a06-853f-4e15-ab61-70e2648f3eac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T17:44:54.986108Z",
     "start_time": "2023-07-22T17:44:50.292836Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def initiate_diagnostic_consultation(diagnostic_df):\n",
    "    diagnostic_df['timestamp'] = pd.to_datetime(diagnostic_df['sessiontimestamp'])\n",
    "    diagnostic_df['date'] = diagnostic_df['timestamp'].dt.date\n",
    "    \n",
    "    diagnostic_df.sort_values(['anonymised_vin', 'date'], kind='mergesort', inplace=True)\n",
    "    diagnostic_df['consultationid'] = (diagnostic_df['anonymised_vin'] != diagnostic_df['anonymised_vin'].shift()).astype(int)\n",
    "    diagnostic_df['consultationid'] += (diagnostic_df['date'] != diagnostic_df['date'].shift()).astype(int)\n",
    "    diagnostic_df['consultationid'] = diagnostic_df['consultationid'].cumsum()\n",
    "    return diagnostic_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26838d72",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Restructuring: Derive Vehicle Data, Diagnostic Reads and Subsequent Diagnostic Actions\n",
    "# !Add description!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cad9aae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T17:44:55.344825Z",
     "start_time": "2023-07-22T17:44:54.999064Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def derive_vehicle_state_data(diagnostic_df):\n",
    "    cols_to_keep = [col for col in diagnostic_df.columns if col not in ['otxsequence', 'date', 'sessionid', 'timestamp']]\n",
    "    vehicle_current_state_df = diagnostic_df[diagnostic_df['otxsequence'] == 'G2725772'][cols_to_keep].copy()\n",
    "\n",
    "    cols_to_keep = ['anonymised_vin', 'consultationid', 'timestamp', 'otxsequence']\n",
    "    diagnostic_actions_df = diagnostic_df[diagnostic_df['otxsequence'] != 'G2725772'][cols_to_keep].copy()\n",
    "\n",
    "    diagnostic_df = vehicle_current_state_df.merge(diagnostic_actions_df, how='inner', on=['anonymised_vin', 'consultationid'])\n",
    "    return diagnostic_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e585c9",
   "metadata": {},
   "source": [
    "## Merge Warranty Data\n",
    "# !Add description!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84521ef0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T17:45:59.413005Z",
     "start_time": "2023-07-22T17:44:55.353844Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def merge_diag_warr_data(diagnostic_df, warranty_df):\n",
    "    diagnostic_df['timestamp'] = pd.to_datetime(diagnostic_df['timestamp'], utc=True)\n",
    "    warranty_df['i_incident_date'] = pd.to_datetime(warranty_df['i_incident_date'], utc=True)\n",
    "\n",
    "    warranty_df = warranty_df.rename(columns={'anonymised_vin': 'warranty_anonymised_vin'})\n",
    "\n",
    "    df_list = []\n",
    "    for idx, row in diagnostic_df.iterrows():\n",
    "        vin = row['anonymised_vin']\n",
    "        diag_time = row['timestamp']\n",
    "\n",
    "        mask = ((warranty_df['warranty_anonymised_vin'] == vin) &\n",
    "                (warranty_df['i_incident_date'] >= diag_time) &\n",
    "                (warranty_df['i_incident_date'] <= diag_time + pd.Timedelta(days=7)))\n",
    "\n",
    "        temp_warranty_df = warranty_df[mask]\n",
    "\n",
    "        for _, warranty_row in temp_warranty_df.iterrows():\n",
    "            merged_row = pd.concat([row, warranty_row])\n",
    "            df_list.append(merged_row)\n",
    "\n",
    "    merged_df = pd.concat(df_list, axis=1).T\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89c2d09",
   "metadata": {},
   "source": [
    "## Feature Engineering: Derive Temporal Features\n",
    "\n",
    "In this section, we perform feature engineering on the 'timestamp' field to extract valuable temporal information about each diagnostic activity and some additional features from the vehicle's 'warrantydate' and 'builddate' data fields. The temporal features we derive are:\n",
    "\n",
    "1. **Year**: The year the diagnostic activity was performed. This can help detect yearly trends in the data.\n",
    "2. **Month**: The month the diagnostic activity was performed. This can help identify any monthly patterns.\n",
    "3. **Day of Week**: The day of the week the diagnostic activity was performed. This can reveal weekly trends, such as certain activities being more common on certain days of the week.\n",
    "4. **Week of Year**: The ISO week number of the year the diagnostic activity was performed. This can provide a more granular view of yearly trends.\n",
    "5. **Time Since Last Activity**: The time in seconds since the last diagnostic activity for each consultation. This can help gauge the frequency of activities.\n",
    "6. **Elapsed Time**: The time in seconds since the first diagnostic activity in each consultation. This can provide insight into the duration of consultations.\n",
    "7. **Season of the Year**: The season (Winter, Spring, Summer, Autumn) when the diagnostic activity was performed. This can help identify seasonal trends, such as certain activities being more common in certain seasons.\n",
    "8. **Age of Vehicle at the Time of Diagnostic Session**: The vehicle's age in years at the time of each diagnostic session. This might be a useful feature because older vehicles might have different diagnostic needs than newer ones.\n",
    "9. **Time in Days since Warranty Started**: This feature might be useful as vehicles might have different diagnostic needs before and after their warranty starts. Also, customers might behave differently before and after their warranty starts.\n",
    "\n",
    "The resulting dataframe now contains several new features that provide additional temporal context about each diagnostic activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e130876",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T17:45:59.625724Z",
     "start_time": "2023-07-22T17:45:59.408333Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def derive_temporal__data_features(merged_df):\n",
    "    merged_df.sort_values(['consultationid', 'timestamp'], inplace=True)\n",
    "\n",
    "    merged_df['year'] = merged_df['timestamp'].dt.year\n",
    "    merged_df['month'] = merged_df['timestamp'].dt.month\n",
    "    merged_df['dayOfWeek'] = merged_df['timestamp'].dt.dayofweek\n",
    "    merged_df['weekOfYear'] = merged_df['timestamp'].dt.isocalendar().week\n",
    "    merged_df['timeSinceLastActivitySec'] = (merged_df.groupby('consultationid')['timestamp'].diff().dt.total_seconds()).fillna(0)\n",
    "    merged_df['elapsedTimeSec'] = merged_df.groupby('consultationid')['timestamp'].transform(lambda x: (x - x.min())).dt.total_seconds()\n",
    "\n",
    "    def month_to_season(month):\n",
    "        if month in [12, 1, 2]:\n",
    "            return 'Winter'\n",
    "        elif month in [3, 4, 5]:\n",
    "            return 'Spring'\n",
    "        elif month in [6, 7, 8]:\n",
    "            return 'Summer'\n",
    "        else:\n",
    "            return 'Autumn'\n",
    "            \n",
    "    merged_df['season'] = merged_df['month'].apply(month_to_season)\n",
    "\n",
    "    merged_df['builddate'] = pd.to_datetime(merged_df['builddate'], format='%Y-%m-%d').dt.tz_localize('UTC')\n",
    "    merged_df['warrantydate'] = pd.to_datetime(merged_df['warrantydate'], format='%Y-%m-%d').dt.tz_localize('UTC')\n",
    "    merged_df['vehicleAgeAtSession'] = (merged_df['timestamp'] - merged_df['builddate']).dt.days / 365\n",
    "    merged_df['daysSinceWarrantyStart'] = (merged_df['timestamp'] - merged_df['warrantydate']).dt.days\n",
    "\n",
    "    merged_df.drop(columns=['builddate', 'warrantydate', 'dtcdescription', 'v_warr_date_event', 'i_p_css_description',\n",
    "                            'i_original_ccc_description', 'i_cpsc_description', 'i_css_description', 'ic_customer_verbatim',\n",
    "                            'ic_technical_verbatim', 'i_incident_date', 'ic_accepted_date', 'warranty_anonymised_vin'],\n",
    "               inplace=True)\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aae3929",
   "metadata": {},
   "source": [
    "## Feature Engineering: Removing Outlier Diagnostic Activities\n",
    "\n",
    "In our dataset, certain diagnostic activities performed by the technicians are extremely common and are recorded in virtually every consultation. While these activities are a routine part of the consultation process, they do not carry significant diagnostic information for our model, and therefore, may not be useful in predicting recommendations. For instance, the 'CONSULTATION_START' activity is logged in every consultation but doesn't contribute meaningful information towards diagnosing a specific vehicle issue.\n",
    "\n",
    "To identify and remove these non-informative activities, we follow a statistical outlier detection approach:\n",
    "\n",
    "1. **Calculate Commonality**: First, we calculate the commonality score for each activity, which is the frequency of the activity divided by the total number of activities.\n",
    "\n",
    "2. **Calculate Mean and Standard Deviation**: We then calculate the mean and standard deviation of these commonality scores.\n",
    "\n",
    "3. **Identify Outliers**: Any activity whose commonality score lies beyond two standard deviations from the mean is considered an outlier. This threshold is based on the empirical rule, which states that for a normal distribution, about 95% of the data lies within two standard deviations of the mean.\n",
    "\n",
    "4. **Remove Outliers**: Finally, we remove these outlier activities from our dataset, leaving us with a set of activities that are varied enough to provide meaningful information for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "766105d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T17:45:59.626777Z",
     "start_time": "2023-07-22T17:45:59.598519Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def remove_outlier_diagnostic_activities(merged_df):\n",
    "    activity_commonality = merged_df.value_counts('otxsequence')/merged_df['otxsequence'].count()\n",
    "    activity_commonality = activity_commonality.reset_index()\n",
    "    activity_commonality.columns = ['otxsequence', 'commonalityScore']\n",
    "\n",
    "    mean = activity_commonality.commonalityScore.mean()\n",
    "    std = activity_commonality.commonalityScore.std()\n",
    "    print(f'MEAN: {mean}  STD: {std}')\n",
    "\n",
    "    lower = mean - (2 * std)\n",
    "    upper = mean + (2 * std)\n",
    "\n",
    "    # Identify the outliers by checking for commonality score less than or greater than lower and upper bounds respectively.\n",
    "    outliers_condition = (activity_commonality.commonalityScore < lower) | (upper < activity_commonality.commonalityScore)\n",
    "    most_common_activities = activity_commonality[outliers_condition]\n",
    "\n",
    "    print(f\"Most common activities with their commonality score (activities to be removed):\\n{most_common_activities}\")\n",
    "    \n",
    "    # Remove identified outlier (the most common) activities\n",
    "    num_records_initial = len(merged_df)\n",
    "    merged_df = merged_df[~merged_df.otxsequence.isin(most_common_activities.otxsequence)]\n",
    "\n",
    "    print(f'Number of records removed: {num_records_initial - len(merged_df)}')\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebf79e0",
   "metadata": {},
   "source": [
    "## Data Cleaning: Removing Duplicate Records\n",
    "\n",
    "In this step of the data preprocessing, we aim to remove any duplicate entries in the dataset.\n",
    "\n",
    "We utilize the `drop_duplicates()` function from pandas library for this purpose. The `inplace=True` parameter ensures that the operation is performed on the dataset directly, without the need to assign the result to a new variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d10460e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T17:45:59.743885Z",
     "start_time": "2023-07-22T17:45:59.648426Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def remove_duplicates(merged_df):\n",
    "    num_records_initial = len(merged_df)\n",
    "    merged_df.drop_duplicates()\n",
    "\n",
    "    print(f'Number of duplicate records removed: {num_records_initial - len(merged_df)}')\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d993c8",
   "metadata": {},
   "source": [
    "## Data Cleaning: Identify and Handle Missing Values\n",
    "# !UPDATE!\n",
    "The isna().sum() call returns a pandas Series with column names as the index and the count of NaN values as the values. We then filter this series to only include columns with more than 0 NaN values and print them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f4494bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T17:45:59.843634Z",
     "start_time": "2023-07-22T17:45:59.807864Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def handle_missing_vals(merged_df):\n",
    "    # Apply 'Unknown' category to all missing values on categorical variables\n",
    "    unordered_cat_cols = ['anonymised_vin', 'consultationid', 'otxsequence', 'model', 'modelyear', 'driver',\n",
    "                          'plant', 'engine', 'transmission', 'module', 'dtcbase', 'faulttype', 'dtcfull',\n",
    "                          'softwarepartnumber', 'hardwarepartnumber', 'i_p_css_code', 'i_original_ccc_code', \n",
    "                          'i_original_vfg_code','i_original_function_code', 'i_original_vrt_code', 'i_current_vfg_code',\n",
    "                          'i_current_function_code', 'i_current_vrt_code',\t'i_cpsc_code', 'i_cpsc_vfg_code',\n",
    "                          'i_css_code', 'v_transmission_code','v_drive_code', 'v_engine_code', 'ic_repair_dealer_id',\n",
    "                          'ic_eng_part_number', 'ic_serv_part_number','ic_part_suffix', 'ic_part_base', 'ic_part_prefix', \n",
    "                          'ic_causal_part_id', 'ic_repair_country_code', 'ic_replaced_yn']\n",
    "\n",
    "    for col in unordered_cat_cols:\n",
    "        merged_df[col] = merged_df[col].fillna('Unknown')\n",
    "\n",
    "        \n",
    "    # If daysSinceWarrantyStart NaN, then warrantydate was empty, meaning it is likely that warranty \n",
    "    # has not started on the vehicle\n",
    "    merged_df['daysSinceWarrantyStart'].fillna(0) \n",
    "    \n",
    "    # If i_mileage NaN, then use current odometer mileage reading (odomiles)\n",
    "    merged_df['i_mileage'].fillna(merged_df['odomiles'], inplace=True)\n",
    "    \n",
    "    na_counts = merged_df.isna().sum()\n",
    "    na_columns = na_counts[na_counts > 0]\n",
    "\n",
    "    if len(na_columns) > 0:\n",
    "        print(f\"Data fields with NaN values:\\n{na_columns}\")\n",
    "        print(f'Total number of records in the DataFrame: {len(merged_df)}')\n",
    "    else:\n",
    "        print('There are no missing values in the DataFrame.')\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0006e446",
   "metadata": {},
   "source": [
    "## Data Normalisation: Standardise Numerical Data\n",
    "\n",
    "In this step, we are standardising the values of the 'elapsedTimeSec', 'timeSinceLastActivitySec', 'odomiles', 'vehicleAgeAtSession', and 'daysSinceWarrantyStart' columns. These columns represent continuous numerical data (temporal data and odometer readings), which we expect to follow a normal-like distribution.\n",
    "\n",
    "We are using sklearn's StandardScaler for this task. This method standardizes features by removing the mean and scaling to unit variance. This transformation helps to achieve properties of a standard normal distribution where the mean (average) of each feature is 0 and the standard deviation is 1.\n",
    "\n",
    "By doing this, we are ensuring that these features have the same scale and thus contributing equally to the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe76ca5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T17:45:59.892461Z",
     "start_time": "2023-07-22T17:45:59.845684Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def standardise_num_data(merged_df):\n",
    "    float_cols = ['elapsedTimeSec', 'timeSinceLastActivitySec', 'odomiles', 'vehicleAgeAtSession',\n",
    "              'daysSinceWarrantyStart', 'i_mileage', 'i_time_in_service', 'i_months_in_service']\n",
    "    for col in float_cols:\n",
    "        merged_df[col] = merged_df[col].astype('float64')\n",
    "    \n",
    "    data_scaler = StandardScaler()\n",
    "    merged_df[float_cols] = data_scaler.fit_transform(merged_df[float_cols])\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3594292c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preprocess and Save Prepared Data\n",
    "\n",
    "Here we are running all the preprocessing steps on the data using parallel computing capabilities and saving the preprocessed data into a CSV file in the as 'prepared_data.csv' in 'data_out' directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acf27941-a813-42fa-a066-6994f2b5ade4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_data(filename):\n",
    "    diagnostic_df = load_data(filename)\n",
    "    print(f'{filename} - Data import complete\\n')\n",
    "    \n",
    "    diagnostic_df = initiate_diagnostic_consultation(diagnostic_df)\n",
    "    print(f'{filename} - Diagnostic consultation initiated\\n')\n",
    "    \n",
    "    diagnostic_df = derive_vehicle_state_data(diagnostic_df)\n",
    "    print(f'{filename} - Vehicle state data derived\\n')\n",
    "    \n",
    "    warranty_df = pd.read_csv(f'data/claims_all.csv', low_memory=False)\n",
    "    merged_df = merge_diag_warr_data(diagnostic_df, warranty_df)\n",
    "    print(f'{filename} - Diagnostic and warranty data merged\\n')\n",
    "    \n",
    "    merged_df = derive_temporal__data_features(merged_df)\n",
    "    print(f'{filename} - Temporal features derived\\n')\n",
    "    \n",
    "    merged_df = handle_missing_vals(merged_df)\n",
    "    print(f'{filename} - Missing values addressed\\n')\n",
    "    \n",
    "    merged_df = remove_duplicates(merged_df)\n",
    "    print(f'{filename} - Duplicates removed\\n')\n",
    "    \n",
    "    merged_df = standardise_num_data(merged_df)\n",
    "    print(f'{filename} - Numerical data normalised\\n')\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "452dfb5a-8d08-4a44-b53e-2b7fc1c1eec0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_csv(df, filename, append=False):\n",
    "    if not os.path.isfile(filename):\n",
    "        df.to_csv(filename, index=False)\n",
    "    if append:\n",
    "        df.to_csv(filename, mode='a', header=False, index=False) # Append to existing CSV\n",
    "    else:\n",
    "        df.to_csv(filename, index=False)          \n",
    "\n",
    "# Process a single chunk and save it to the CSV\n",
    "def process_and_save_chunk(function, chunk_file):\n",
    "    try:\n",
    "        result = process_data(chunk_file)\n",
    "        with lock:\n",
    "            save_csv(result, 'data_out/prepared_chunks_data.csv', True)\n",
    "            print(f'Processed and saved chunk: {chunk_file}\\n')\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Process and save chunks in parallel\n",
    "def parallel_process_and_save_chunks(function, chunk_files):\n",
    "    print(f'CPU count: {mp.cpu_count()}\\n')\n",
    "    global lock\n",
    "    lock = mp.Lock()  # Create a lock object to prevent concurrent writes to the same file\n",
    "    with mp.Pool() as pool:\n",
    "        pool.starmap(process_and_save_chunk, [(function, chunk_file) for chunk_file in chunk_files]) \n",
    "    print('All chunks processed and saved.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d5a353-7605-428b-a778-7b9d6c77fe06",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU count: 36\n",
      "\n",
      "diagnostic_chunk2.csv - Data import complete\n",
      "\n",
      "diagnostic_chunk3.csv - Data import complete\n",
      "\n",
      "diagnostic_chunk2.csv - Diagnostic consultation initiated\n",
      "\n",
      "diagnostic_chunk3.csv - Diagnostic consultation initiated\n",
      "\n",
      "diagnostic_chunk2.csv - Vehicle state data derived\n",
      "\n",
      "diagnostic_chunk3.csv - Vehicle state data derived\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get list of diagnostic data chunk files\n",
    "# chunk_files = [file for file in os.listdir('data/chunks_2m') if file.endswith('.csv')]\n",
    "chunk_files = ['diagnostic_chunk3.csv', 'diagnostic_chunk2.csv']\n",
    "# Run\n",
    "parallel_process_and_save_chunks(process_data, chunk_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb58797-a3c4-4a16-b250-c5502fb4c6f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prepared_data_df = load_data('data_out/prepared_chunks_data.csv')\n",
    "\n",
    "prepared_data_df = remove_outlier_diagnostic_activities(prepared_data_df)\n",
    "print(f'{filename} - Outlier diagnostic activities removed\\n')\n",
    "\n",
    "prepared_data_df = remove_duplicates(prepared_data_df)\n",
    "print(f'{filename} - Duplicates removed\\n')\n",
    "\n",
    "save_csv(prepared_data_df, 'data_out/prepared_data.csv', False)\n",
    "print('Done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
